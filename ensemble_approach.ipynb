{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def ensemble_approach(repeat_accuracy,*test_X,test_y,model):\n",
    "    repeat_accuracy=np.array(repeat_accuracy).mean(1)\n",
    "    index = np.where(repeat_accuracy == np.amax(repeat_accuracy))[0][0]\n",
    "    print(index)\n",
    "  \n",
    "    ens_y_pred=[]\n",
    "    ens_y_pred_proba=[]\n",
    "\n",
    "    for i in range(5):\n",
    "        weights_file='weights/' + model.name + str(index) + '_' + str(i) +'_best_pretrained_weights.npy'\n",
    "        model.load_weights(weights_file)\n",
    "        prediction=model.predict([test_X[0], test_X[1], test_X[2]])\n",
    "        y_pred = (prediction > 0.5).argmax(axis=1)\n",
    "        y_pred_proba=prediction[:, 1]\n",
    "        ens_y_pred.append(y_pred)\n",
    "        ens_y_pred_proba.append(y_pred_proba)\n",
    "        \n",
    "    new_ens_y_pred=np.transpose(np.array(ens_y_pred),(1,0))\n",
    "    new_ens_y_pred=scipy.stats.mode(new_ens_y_pred,axis=1)[0]\n",
    "    ens_y_true=test_y\n",
    "    ens_matrix=metrics.confusion_matrix(ens_y_true,new_ens_y_pred)\n",
    "    print(ens_matrix)\n",
    "    #accuracy\n",
    "    total = sum(sum(ens_matrix))\n",
    "    ens_accuracy = (ens_matrix[0, 0] + ens_matrix[1, 1]) / total\n",
    "\n",
    "    #sensitivity & specificity\n",
    "    ens_specificity=ens_matrix[0, 0] / (ens_matrix[0, 0] + ens_matrix[0, 1])\n",
    "    ens_sensitivity=ens_matrix[1, 1] / (ens_matrix[1, 0] + ens_matrix[1, 1])\n",
    "\n",
    "    print(\"%.2f%% \" % (np.mean(ens_accuracy)*100))\n",
    "    print(\"%.2f%% \" % (np.mean(ens_sensitivity)*100))\n",
    "    print(\"%.2f%% \" % (np.mean(ens_specificity)*100))\n",
    "    \n",
    "    roc_y_pred=np.zeros(np.array(ens_y_pred).shape)\n",
    "    for i in range (roc_y_pred.shape[0]):\n",
    "        for j in range (roc_y_pred.shape[1]):\n",
    "            roc_y_pred[i][j]=1-abs(ens_y_pred[i][j]-new_ens_y_pred[j])\n",
    "    \n",
    "    roc_y_pred_proba=ens_y_pred_proba*roc_y_pred\n",
    "    roc_y_pred_proba=np.sum(roc_y_pred_proba, axis=0)/np.sum(roc_y_pred,axis=0)\n",
    "    \n",
    "    test_y_probability=roc_y_pred_proba\n",
    "    test_y_label=ens_y_true\n",
    "    return test_y_probability,test_y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def plot_roc_curve(y_true,y_pred):\n",
    "    y_true=np.array(y_true).flatten()\n",
    "    y_pred=np.array(y_pred).flatten()\n",
    "    auc = roc_auc_score(y_true,y_pred)\n",
    "    print('AUC: %.3f' % auc)\n",
    "\n",
    "    # calculate roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true,y_pred)\n",
    "    # plot no skill\n",
    "    pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(ens_y_true,roc_y_pred_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
